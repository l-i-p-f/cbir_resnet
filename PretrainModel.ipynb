{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general both transfer learning methods follow the same few steps:\n",
    "\n",
    "- Initialize the pretrained model\n",
    "- Reshape the final layer(s) to have the same number of outputs as the number of classes in the new dataset\n",
    "- Define for the optimization algorithm which parameters we want to update during training\n",
    "- Run the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  0.4.1\n",
      "Torchvision Version:  0.2.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "import pickle as pk\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"/data1/lipf/cifar-100-python/\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 100\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 512\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Validation Code\n",
    "\n",
    "Training and validation. \n",
    "\n",
    "Input : a PyTorch model, a dictionary of dataloaders, a loss function, an optimizer, a specified number of epochs to train and validate for, and a boolean flag for when the model is an Inception model. \n",
    "\n",
    "The is_inception flag is used to accomodate the Inception v3 model. \n",
    "\n",
    "The function trains for the specified number of epochs and **after each epoch runs a full validation step**. It also keeps track of the best performing model (in terms of validation accuracy), and at the end of training returns the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            iters = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                iters += inputs.size(0)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # torch.max(a,1) 返回每一行中最大值的那个元素，且返回其索引，１表示行\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    # _, preds = torch.topk(outputs, 5)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                # 通过.item() 从0维张量中获得 python number.\n",
    "                # 如果loss不转为　python number　再累加，内存消耗会一直在增加。因为0维张量会增加梯度的计算历史\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                # 转置，然后扩展\n",
    "                # running_corrects += torch.sum(preds == labels.data.view(1,-1).t().expand_as(preds))\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Set Model Parameters’ .requires_grad attribute\n",
    "\n",
    "This helper function sets the .requires_grad attribute of the parameters in the model to False when we are feature extracting. By default, when we load a pretrained model all of the parameters have .requires_grad=True, which is fine if we are training from scratch or finetuning. \n",
    "\n",
    "However, if we are feature extracting and only want to compute gradients for the newly initialized layer then we want all of the other parameters to not require gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE AND RESHAPE THE NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When feature extracting, we only want to update the parameters of the last layer, or in other words, we only want to update the parameters for the layer(s) we are reshaping. Therefore, we do not need to compute the gradients of the parameters that we are not changing, so for efficiency we set the .requires_grad attribute to False. This is important because by default, this attribute is set to True. Then, when we initialize the new layer and by default the new parameters have .requires_grad=True so only the new layer’s parameters will be updated. When we are finetuning we can leave all of the .required_grad’s set to the default of True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=2048, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet101\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet101(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.avgpool = nn.AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 32    \n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:6\n"
     ]
    }
   ],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataLoader(is_train='train', cuda=True, batch_size=512, shuffle=True):\n",
    "        if is_train == 'train':\n",
    "            trans = [transforms.RandomHorizontalFlip(),\n",
    "                     transforms.RandomCrop(32, padding=4),\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    "            trans = transforms.Compose(trans)\n",
    "            data_set = datasets.CIFAR100(root='./', train=True, transform=trans)\n",
    "        else:\n",
    "            trans = [transforms.ToTensor(),\n",
    "                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    "            trans = transforms.Compose(trans)\n",
    "            data_set = datasets.CIFAR100('./', train=False, transform=trans)\n",
    "\n",
    "        return data_set\n",
    "\n",
    "dataset = {}\n",
    "for phase in ['train', 'val']:\n",
    "    dataset[phase] = dataLoader(phase)\n",
    "\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(dataset[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE THE OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t conv1.weight\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t layer1.0.conv1.weight\n",
      "\t layer1.0.bn1.weight\n",
      "\t layer1.0.bn1.bias\n",
      "\t layer1.0.conv2.weight\n",
      "\t layer1.0.bn2.weight\n",
      "\t layer1.0.bn2.bias\n",
      "\t layer1.0.conv3.weight\n",
      "\t layer1.0.bn3.weight\n",
      "\t layer1.0.bn3.bias\n",
      "\t layer1.0.downsample.0.weight\n",
      "\t layer1.0.downsample.1.weight\n",
      "\t layer1.0.downsample.1.bias\n",
      "\t layer1.1.conv1.weight\n",
      "\t layer1.1.bn1.weight\n",
      "\t layer1.1.bn1.bias\n",
      "\t layer1.1.conv2.weight\n",
      "\t layer1.1.bn2.weight\n",
      "\t layer1.1.bn2.bias\n",
      "\t layer1.1.conv3.weight\n",
      "\t layer1.1.bn3.weight\n",
      "\t layer1.1.bn3.bias\n",
      "\t layer1.2.conv1.weight\n",
      "\t layer1.2.bn1.weight\n",
      "\t layer1.2.bn1.bias\n",
      "\t layer1.2.conv2.weight\n",
      "\t layer1.2.bn2.weight\n",
      "\t layer1.2.bn2.bias\n",
      "\t layer1.2.conv3.weight\n",
      "\t layer1.2.bn3.weight\n",
      "\t layer1.2.bn3.bias\n",
      "\t layer2.0.conv1.weight\n",
      "\t layer2.0.bn1.weight\n",
      "\t layer2.0.bn1.bias\n",
      "\t layer2.0.conv2.weight\n",
      "\t layer2.0.bn2.weight\n",
      "\t layer2.0.bn2.bias\n",
      "\t layer2.0.conv3.weight\n",
      "\t layer2.0.bn3.weight\n",
      "\t layer2.0.bn3.bias\n",
      "\t layer2.0.downsample.0.weight\n",
      "\t layer2.0.downsample.1.weight\n",
      "\t layer2.0.downsample.1.bias\n",
      "\t layer2.1.conv1.weight\n",
      "\t layer2.1.bn1.weight\n",
      "\t layer2.1.bn1.bias\n",
      "\t layer2.1.conv2.weight\n",
      "\t layer2.1.bn2.weight\n",
      "\t layer2.1.bn2.bias\n",
      "\t layer2.1.conv3.weight\n",
      "\t layer2.1.bn3.weight\n",
      "\t layer2.1.bn3.bias\n",
      "\t layer2.2.conv1.weight\n",
      "\t layer2.2.bn1.weight\n",
      "\t layer2.2.bn1.bias\n",
      "\t layer2.2.conv2.weight\n",
      "\t layer2.2.bn2.weight\n",
      "\t layer2.2.bn2.bias\n",
      "\t layer2.2.conv3.weight\n",
      "\t layer2.2.bn3.weight\n",
      "\t layer2.2.bn3.bias\n",
      "\t layer2.3.conv1.weight\n",
      "\t layer2.3.bn1.weight\n",
      "\t layer2.3.bn1.bias\n",
      "\t layer2.3.conv2.weight\n",
      "\t layer2.3.bn2.weight\n",
      "\t layer2.3.bn2.bias\n",
      "\t layer2.3.conv3.weight\n",
      "\t layer2.3.bn3.weight\n",
      "\t layer2.3.bn3.bias\n",
      "\t layer3.0.conv1.weight\n",
      "\t layer3.0.bn1.weight\n",
      "\t layer3.0.bn1.bias\n",
      "\t layer3.0.conv2.weight\n",
      "\t layer3.0.bn2.weight\n",
      "\t layer3.0.bn2.bias\n",
      "\t layer3.0.conv3.weight\n",
      "\t layer3.0.bn3.weight\n",
      "\t layer3.0.bn3.bias\n",
      "\t layer3.0.downsample.0.weight\n",
      "\t layer3.0.downsample.1.weight\n",
      "\t layer3.0.downsample.1.bias\n",
      "\t layer3.1.conv1.weight\n",
      "\t layer3.1.bn1.weight\n",
      "\t layer3.1.bn1.bias\n",
      "\t layer3.1.conv2.weight\n",
      "\t layer3.1.bn2.weight\n",
      "\t layer3.1.bn2.bias\n",
      "\t layer3.1.conv3.weight\n",
      "\t layer3.1.bn3.weight\n",
      "\t layer3.1.bn3.bias\n",
      "\t layer3.2.conv1.weight\n",
      "\t layer3.2.bn1.weight\n",
      "\t layer3.2.bn1.bias\n",
      "\t layer3.2.conv2.weight\n",
      "\t layer3.2.bn2.weight\n",
      "\t layer3.2.bn2.bias\n",
      "\t layer3.2.conv3.weight\n",
      "\t layer3.2.bn3.weight\n",
      "\t layer3.2.bn3.bias\n",
      "\t layer3.3.conv1.weight\n",
      "\t layer3.3.bn1.weight\n",
      "\t layer3.3.bn1.bias\n",
      "\t layer3.3.conv2.weight\n",
      "\t layer3.3.bn2.weight\n",
      "\t layer3.3.bn2.bias\n",
      "\t layer3.3.conv3.weight\n",
      "\t layer3.3.bn3.weight\n",
      "\t layer3.3.bn3.bias\n",
      "\t layer3.4.conv1.weight\n",
      "\t layer3.4.bn1.weight\n",
      "\t layer3.4.bn1.bias\n",
      "\t layer3.4.conv2.weight\n",
      "\t layer3.4.bn2.weight\n",
      "\t layer3.4.bn2.bias\n",
      "\t layer3.4.conv3.weight\n",
      "\t layer3.4.bn3.weight\n",
      "\t layer3.4.bn3.bias\n",
      "\t layer3.5.conv1.weight\n",
      "\t layer3.5.bn1.weight\n",
      "\t layer3.5.bn1.bias\n",
      "\t layer3.5.conv2.weight\n",
      "\t layer3.5.bn2.weight\n",
      "\t layer3.5.bn2.bias\n",
      "\t layer3.5.conv3.weight\n",
      "\t layer3.5.bn3.weight\n",
      "\t layer3.5.bn3.bias\n",
      "\t layer3.6.conv1.weight\n",
      "\t layer3.6.bn1.weight\n",
      "\t layer3.6.bn1.bias\n",
      "\t layer3.6.conv2.weight\n",
      "\t layer3.6.bn2.weight\n",
      "\t layer3.6.bn2.bias\n",
      "\t layer3.6.conv3.weight\n",
      "\t layer3.6.bn3.weight\n",
      "\t layer3.6.bn3.bias\n",
      "\t layer3.7.conv1.weight\n",
      "\t layer3.7.bn1.weight\n",
      "\t layer3.7.bn1.bias\n",
      "\t layer3.7.conv2.weight\n",
      "\t layer3.7.bn2.weight\n",
      "\t layer3.7.bn2.bias\n",
      "\t layer3.7.conv3.weight\n",
      "\t layer3.7.bn3.weight\n",
      "\t layer3.7.bn3.bias\n",
      "\t layer3.8.conv1.weight\n",
      "\t layer3.8.bn1.weight\n",
      "\t layer3.8.bn1.bias\n",
      "\t layer3.8.conv2.weight\n",
      "\t layer3.8.bn2.weight\n",
      "\t layer3.8.bn2.bias\n",
      "\t layer3.8.conv3.weight\n",
      "\t layer3.8.bn3.weight\n",
      "\t layer3.8.bn3.bias\n",
      "\t layer3.9.conv1.weight\n",
      "\t layer3.9.bn1.weight\n",
      "\t layer3.9.bn1.bias\n",
      "\t layer3.9.conv2.weight\n",
      "\t layer3.9.bn2.weight\n",
      "\t layer3.9.bn2.bias\n",
      "\t layer3.9.conv3.weight\n",
      "\t layer3.9.bn3.weight\n",
      "\t layer3.9.bn3.bias\n",
      "\t layer3.10.conv1.weight\n",
      "\t layer3.10.bn1.weight\n",
      "\t layer3.10.bn1.bias\n",
      "\t layer3.10.conv2.weight\n",
      "\t layer3.10.bn2.weight\n",
      "\t layer3.10.bn2.bias\n",
      "\t layer3.10.conv3.weight\n",
      "\t layer3.10.bn3.weight\n",
      "\t layer3.10.bn3.bias\n",
      "\t layer3.11.conv1.weight\n",
      "\t layer3.11.bn1.weight\n",
      "\t layer3.11.bn1.bias\n",
      "\t layer3.11.conv2.weight\n",
      "\t layer3.11.bn2.weight\n",
      "\t layer3.11.bn2.bias\n",
      "\t layer3.11.conv3.weight\n",
      "\t layer3.11.bn3.weight\n",
      "\t layer3.11.bn3.bias\n",
      "\t layer3.12.conv1.weight\n",
      "\t layer3.12.bn1.weight\n",
      "\t layer3.12.bn1.bias\n",
      "\t layer3.12.conv2.weight\n",
      "\t layer3.12.bn2.weight\n",
      "\t layer3.12.bn2.bias\n",
      "\t layer3.12.conv3.weight\n",
      "\t layer3.12.bn3.weight\n",
      "\t layer3.12.bn3.bias\n",
      "\t layer3.13.conv1.weight\n",
      "\t layer3.13.bn1.weight\n",
      "\t layer3.13.bn1.bias\n",
      "\t layer3.13.conv2.weight\n",
      "\t layer3.13.bn2.weight\n",
      "\t layer3.13.bn2.bias\n",
      "\t layer3.13.conv3.weight\n",
      "\t layer3.13.bn3.weight\n",
      "\t layer3.13.bn3.bias\n",
      "\t layer3.14.conv1.weight\n",
      "\t layer3.14.bn1.weight\n",
      "\t layer3.14.bn1.bias\n",
      "\t layer3.14.conv2.weight\n",
      "\t layer3.14.bn2.weight\n",
      "\t layer3.14.bn2.bias\n",
      "\t layer3.14.conv3.weight\n",
      "\t layer3.14.bn3.weight\n",
      "\t layer3.14.bn3.bias\n",
      "\t layer3.15.conv1.weight\n",
      "\t layer3.15.bn1.weight\n",
      "\t layer3.15.bn1.bias\n",
      "\t layer3.15.conv2.weight\n",
      "\t layer3.15.bn2.weight\n",
      "\t layer3.15.bn2.bias\n",
      "\t layer3.15.conv3.weight\n",
      "\t layer3.15.bn3.weight\n",
      "\t layer3.15.bn3.bias\n",
      "\t layer3.16.conv1.weight\n",
      "\t layer3.16.bn1.weight\n",
      "\t layer3.16.bn1.bias\n",
      "\t layer3.16.conv2.weight\n",
      "\t layer3.16.bn2.weight\n",
      "\t layer3.16.bn2.bias\n",
      "\t layer3.16.conv3.weight\n",
      "\t layer3.16.bn3.weight\n",
      "\t layer3.16.bn3.bias\n",
      "\t layer3.17.conv1.weight\n",
      "\t layer3.17.bn1.weight\n",
      "\t layer3.17.bn1.bias\n",
      "\t layer3.17.conv2.weight\n",
      "\t layer3.17.bn2.weight\n",
      "\t layer3.17.bn2.bias\n",
      "\t layer3.17.conv3.weight\n",
      "\t layer3.17.bn3.weight\n",
      "\t layer3.17.bn3.bias\n",
      "\t layer3.18.conv1.weight\n",
      "\t layer3.18.bn1.weight\n",
      "\t layer3.18.bn1.bias\n",
      "\t layer3.18.conv2.weight\n",
      "\t layer3.18.bn2.weight\n",
      "\t layer3.18.bn2.bias\n",
      "\t layer3.18.conv3.weight\n",
      "\t layer3.18.bn3.weight\n",
      "\t layer3.18.bn3.bias\n",
      "\t layer3.19.conv1.weight\n",
      "\t layer3.19.bn1.weight\n",
      "\t layer3.19.bn1.bias\n",
      "\t layer3.19.conv2.weight\n",
      "\t layer3.19.bn2.weight\n",
      "\t layer3.19.bn2.bias\n",
      "\t layer3.19.conv3.weight\n",
      "\t layer3.19.bn3.weight\n",
      "\t layer3.19.bn3.bias\n",
      "\t layer3.20.conv1.weight\n",
      "\t layer3.20.bn1.weight\n",
      "\t layer3.20.bn1.bias\n",
      "\t layer3.20.conv2.weight\n",
      "\t layer3.20.bn2.weight\n",
      "\t layer3.20.bn2.bias\n",
      "\t layer3.20.conv3.weight\n",
      "\t layer3.20.bn3.weight\n",
      "\t layer3.20.bn3.bias\n",
      "\t layer3.21.conv1.weight\n",
      "\t layer3.21.bn1.weight\n",
      "\t layer3.21.bn1.bias\n",
      "\t layer3.21.conv2.weight\n",
      "\t layer3.21.bn2.weight\n",
      "\t layer3.21.bn2.bias\n",
      "\t layer3.21.conv3.weight\n",
      "\t layer3.21.bn3.weight\n",
      "\t layer3.21.bn3.bias\n",
      "\t layer3.22.conv1.weight\n",
      "\t layer3.22.bn1.weight\n",
      "\t layer3.22.bn1.bias\n",
      "\t layer3.22.conv2.weight\n",
      "\t layer3.22.bn2.weight\n",
      "\t layer3.22.bn2.bias\n",
      "\t layer3.22.conv3.weight\n",
      "\t layer3.22.bn3.weight\n",
      "\t layer3.22.bn3.bias\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.conv3.weight\n",
      "\t layer4.0.bn3.weight\n",
      "\t layer4.0.bn3.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t layer4.1.conv3.weight\n",
      "\t layer4.1.bn3.weight\n",
      "\t layer4.1.bn3.bias\n",
      "\t layer4.2.conv1.weight\n",
      "\t layer4.2.bn1.weight\n",
      "\t layer4.2.bn1.bias\n",
      "\t layer4.2.conv2.weight\n",
      "\t layer4.2.bn2.weight\n",
      "\t layer4.2.bn2.bias\n",
      "\t layer4.2.conv3.weight\n",
      "\t layer4.2.bn3.weight\n",
      "\t layer4.2.bn3.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "for name,param in model_ft.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN TRAINING AND VALIDATION STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 2.7881 Acc: 0.2998\n",
      "val Loss: 2.2118 Acc: 0.4140\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.9632 Acc: 0.4684\n",
      "val Loss: 1.9228 Acc: 0.4785\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.6988 Acc: 0.5269\n",
      "val Loss: 1.8272 Acc: 0.5140\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 1.5244 Acc: 0.5695\n",
      "val Loss: 1.7238 Acc: 0.5349\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 1.3910 Acc: 0.6003\n",
      "val Loss: 1.7071 Acc: 0.5406\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.2800 Acc: 0.6299\n",
      "val Loss: 1.6287 Acc: 0.5618\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 1.3071 Acc: 0.6278\n",
      "val Loss: 16.3962 Acc: 0.4737\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.5268 Acc: 0.5715\n",
      "val Loss: 1.6000 Acc: 0.5627\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.1628 Acc: 0.6577\n",
      "val Loss: 1.6039 Acc: 0.5735\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.0397 Acc: 0.6887\n",
      "val Loss: 1.8014 Acc: 0.5290\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.0577 Acc: 0.6858\n",
      "val Loss: 2.3358 Acc: 0.5018\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.9745 Acc: 0.7029\n",
      "val Loss: 1.7835 Acc: 0.5774\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.8127 Acc: 0.7485\n",
      "val Loss: 1.6803 Acc: 0.5794\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.7725 Acc: 0.7604\n",
      "val Loss: 1.8466 Acc: 0.5636\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.8839 Acc: 0.7298\n",
      "val Loss: 1.6310 Acc: 0.5880\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.6449 Acc: 0.7983\n",
      "val Loss: 1.6836 Acc: 0.5874\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.6283 Acc: 0.8019\n",
      "val Loss: 1.7201 Acc: 0.5902\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.5290 Acc: 0.8318\n",
      "val Loss: 1.7429 Acc: 0.5958\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.4684 Acc: 0.8500\n",
      "val Loss: 1.7849 Acc: 0.5936\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.4706 Acc: 0.8481\n",
      "val Loss: 1.8192 Acc: 0.5916\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.3982 Acc: 0.8717\n",
      "val Loss: 1.8197 Acc: 0.6067\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.4906 Acc: 0.8430\n",
      "val Loss: 1.8904 Acc: 0.5810\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.3484 Acc: 0.8867\n",
      "val Loss: 1.8835 Acc: 0.6002\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.4089 Acc: 0.8698\n",
      "val Loss: 2.0503 Acc: 0.5619\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.5052 Acc: 0.8414\n",
      "val Loss: 1.9996 Acc: 0.5691\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.3655 Acc: 0.8825\n",
      "val Loss: 1.9698 Acc: 0.5878\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.2941 Acc: 0.9059\n",
      "val Loss: 1.9798 Acc: 0.5969\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.2482 Acc: 0.9211\n",
      "val Loss: 2.0396 Acc: 0.5999\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.2042 Acc: 0.9340\n",
      "val Loss: 2.0897 Acc: 0.6000\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.1808 Acc: 0.9414\n",
      "val Loss: 2.1044 Acc: 0.5963\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.1741 Acc: 0.9437\n",
      "val Loss: 2.1839 Acc: 0.5931\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.1694 Acc: 0.9455\n",
      "val Loss: 2.2758 Acc: 0.5964\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.2080 Acc: 0.9328\n",
      "val Loss: 2.3353 Acc: 0.5859\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.1748 Acc: 0.9434\n",
      "val Loss: 2.7150 Acc: 0.5950\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.1539 Acc: 0.9493\n",
      "val Loss: 2.8298 Acc: 0.5502\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.4636 Acc: 0.8557\n",
      "val Loss: 2.2782 Acc: 0.5685\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.2149 Acc: 0.9303\n",
      "val Loss: 2.1566 Acc: 0.5911\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.1495 Acc: 0.9522\n",
      "val Loss: 2.2512 Acc: 0.5943\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.2578 Acc: 0.9196\n",
      "val Loss: 2.6844 Acc: 0.5466\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.2809 Acc: 0.9087\n",
      "val Loss: 2.2708 Acc: 0.5758\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.1610 Acc: 0.9471\n",
      "val Loss: 2.2063 Acc: 0.5989\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.1357 Acc: 0.9554\n",
      "val Loss: 2.3024 Acc: 0.5927\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.1213 Acc: 0.9613\n",
      "val Loss: 2.3594 Acc: 0.5920\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.1015 Acc: 0.9677\n",
      "val Loss: 2.3521 Acc: 0.5940\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0957 Acc: 0.9694\n",
      "val Loss: 2.3972 Acc: 0.5964\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.1079 Acc: 0.9653\n",
      "val Loss: 2.4072 Acc: 0.5934\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0961 Acc: 0.9689\n",
      "val Loss: 2.4124 Acc: 0.5880\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.3246 Acc: 0.8988\n",
      "val Loss: 2.4458 Acc: 0.5787\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.1387 Acc: 0.9554\n",
      "val Loss: 2.6825 Acc: 0.5892\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0954 Acc: 0.9705\n",
      "val Loss: 2.3821 Acc: 0.5986\n",
      "\n",
      "Training complete in 24m 13s\n",
      "Best val Acc: 0.606700\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save parameters\n",
    "save_path = './acc_6067.pth'\n",
    "torch.save(model_ft.cpu().state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotacc(hist, num_epochs):\n",
    "    ohist = []\n",
    "\n",
    "    ohist = [h.cpu().numpy() for h in hist]\n",
    "\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Training Epochs\")\n",
    "    plt.ylabel(\"Validation Accuracy\")\n",
    "    plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "    plt.ylim((0,1.))\n",
    "    plt.xticks(np.arange(1, num_epochs+1, 5.0))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngoiIxCgpiIhIzP8Hk3MYJ4dWWQoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9583e56320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotacc(hist, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
